{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A solução foi implementada no google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWeJfY879Fk_",
    "outputId": "1618b943-2155-4e3e-8ad6-796246bf2162",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação dos principais pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B0LUSPXnWSM1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import os,glob\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import listdir,makedirs\n",
    "from os.path import isfile,join\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaneando todos os arquivos jpg da pasta images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1jPN830LWpYV"
   },
   "outputs": [],
   "source": [
    "def scan_files(path,regra): \n",
    "    resultados = list(Path(path).rglob(str(regra)))\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Sa6oFuIaNetm"
   },
   "outputs": [],
   "source": [
    "filenames = scan_files(path = '/content/drive/MyDrive/archive/images/images',regra = \"*.[jJ][pP][gG]\") #existem 3 arquivos com extensão diferente de .jpg, então iremos ignora-los"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma vez que a cor não é importante para diferenciar se é cachorro ou gato, iremos utilizar imagens em escala de cinza a fim de reduzir a complexidade e podendo utilizar-se de resoluções maiores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria a pasta destino das imagens cinzas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nYBKo4KrQC-Z"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/archive/images/images' # Source Folder\n",
    "dstpath = '/content/drive/MyDrive/archive/images/gray_images' # Destination Folder\n",
    "try:\n",
    "    makedirs(dstpath)\n",
    "except:\n",
    "    print (\"Directory already exist, images will be written in same folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converte para escala de cinza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKvgJPK0QUSJ",
    "outputId": "b6e0f10f-c0d7-4b92-8fd3-819a3b2a3d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/archive/images/images/Egyptian_Mau_139.jpg is not converted\n",
      "/content/drive/MyDrive/archive/images/images/Egyptian_Mau_145.jpg is not converted\n",
      "/content/drive/MyDrive/archive/images/images/Egyptian_Mau_167.jpg is not converted\n",
      "/content/drive/MyDrive/archive/images/images/Egyptian_Mau_177.jpg is not converted\n",
      "/content/drive/MyDrive/archive/images/images/Egyptian_Mau_191.jpg is not converted\n",
      "/content/drive/MyDrive/archive/images/images/Abyssinian_34.jpg is not converted\n",
      "Fim da conversão\n"
     ]
    }
   ],
   "source": [
    "# Folder won't used\n",
    "files = filenames\n",
    "altura_total = 0\n",
    "largura_total = 0\n",
    "count = 0\n",
    "for count, image in enumerate(files):\n",
    "    try:\n",
    "        original_image = image\n",
    "        image = str(image)\n",
    "        img = cv2.imread(image)\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "        dstPath = join(dstpath,original_image.name)\n",
    "        cv2.imwrite(dstPath,gray)\n",
    "        altura,largura,ch = img.shape\n",
    "        altura_total = altura_total +  altura\n",
    "        largura_total = largura_total + largura\n",
    "    except:\n",
    "        pass\n",
    "        print (\"{} is not converted\".format(image))\n",
    "\n",
    "altura_media = int(altura_total/(count+1)) #390\n",
    "largura_media = int(largura_total/(count+1)) #436\n",
    "print('Fim da conversão')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A fim de não gerar muitas distorções nas imagens, iremos utilizar a resolução média das imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KwTJyvV6sor",
    "outputId": "46eab947-2e78-40e6-bdbf-bc7459d1a4d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 436)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "altura_media,largura_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iIJVjz_6ajfL"
   },
   "outputs": [],
   "source": [
    "#Propriedades das imagens\n",
    "Image_Width=largura_media\n",
    "Image_Height=altura_media\n",
    "Image_Size=(Image_Width,Image_Height)\n",
    "Image_Channels= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaneando o caminho das imagens em escala de cinza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gDRFHU3_QZUF"
   },
   "outputs": [],
   "source": [
    "filenames = scan_files(path = '/content/drive/MyDrive/archive/images/gray_images',regra = \"*.[jJ][pP][gG]\") #existem 3 arquivos com extensão diferente de .jpg, então iremos ignora-los"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando os rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B50Nl2GscfmP"
   },
   "outputs": [],
   "source": [
    "cats = 0\n",
    "dogs = 0\n",
    "#Preparando o dataset para o treinamento do modelo:\n",
    "categories=[]\n",
    "for f_name in filenames:\n",
    "    file_name = f_name.name\n",
    "    file_name = str(file_name)[:str(file_name).find('.')]\n",
    "\n",
    "    if file_name.istitle():\n",
    "        categories.append(0) # cat = 0\n",
    "        cats +=1\n",
    "    else:\n",
    "        categories.append(1) # dog = 1\n",
    "        dogs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HU1ujmVK6fxW",
    "outputId": "ae8bd4b9-5449-410b-e9f4-9f6dc7ceaf20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2394, 4990)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats,dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_VzfbGz64rS",
    "outputId": "65a0ff5f-226e-4400-bf1f-9ffcb818d1cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7384"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats+dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "R2WmEXmmkKSW"
   },
   "outputs": [],
   "source": [
    "str_filenames = []\n",
    "for f_name in filenames:\n",
    "    str_filename = '/content/drive/MyDrive/archive/images/gray_images/'+f_name.name\n",
    "    str_filenames.append(str_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando a estrutura da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ni3bzzPMhbvH"
   },
   "outputs": [],
   "source": [
    "#Criando o modelo de rede neural:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D, Dropout,Flatten,Dense,Activation, BatchNormalization\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(Image_Width,Image_Height,Image_Channels)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dtzxskzth-dk",
    "outputId": "43219003-6fa3-4247-ad10-29f5739e13cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 434, 388, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 434, 388, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 217, 194, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 217, 194, 32)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 215, 192, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 215, 192, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 107, 96, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 107, 96, 64)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 105, 94, 128)      73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 105, 94, 128)     512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 52, 47, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 52, 47, 128)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 312832)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               160170496 \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,267,138\n",
      "Trainable params: 160,265,666\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Analisando as camadas do modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dF7hu9IaiJjX"
   },
   "outputs": [],
   "source": [
    "#Definindo os callbacks e o LR\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint\n",
    "\n",
    "earlystop = EarlyStopping(patience = 20)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy',patience = 2,verbose = 1,factor = 0.5,min_lr = 0.00001)\n",
    "\n",
    "checkpoint = ModelCheckpoint('/content/drive/MyDrive/best_model_gray.h5', monitor= 'val_accuracy', save_best_only=True) \n",
    "\n",
    "callbacks = [earlystop,learning_rate_reduction,checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratificando pela categoria com menor casos. Nesse caso, a categoria gatos que possuem menos amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Jw434rbiyboz"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\n",
    "    'filename':str_filenames,\n",
    "    'category':categories\n",
    "})\n",
    "df = df.groupby('category', group_keys=False).apply(lambda x: x.sample(min(len(x), min(cats,dogs)))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitando entre o conjunto de treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WoOeadSsxjEn"
   },
   "outputs": [],
   "source": [
    "#Splitando e ajustando os dfs\n",
    "df[\"category\"] = df[\"category\"].replace({0:'cat',1:'dog'})\n",
    "train_df,validate_df = train_test_split(df,test_size=0.20, random_state=42,shuffle=True) #, stratify = df['category']\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = validate_df.reset_index(drop=True)\n",
    "total_train=train_df.shape[0]\n",
    "total_validate=validate_df.shape[0]\n",
    "batch_size= 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLLibjLbidGi",
    "outputId": "695f49cc-e070-4597-c030-7ddd29b6452d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3830 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Gerando a base de treino\n",
    "train_datagen = ImageDataGenerator(rotation_range=15,\n",
    "                                rescale=1/255,\n",
    "                                shear_range=0.1,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1)\n",
    "train_generator = train_datagen.flow_from_dataframe(train_df,\n",
    "                                                 \"/content/drive/MyDrive/archive/images/gray_images/\",\n",
    "                                                 x_col='filename',y_col='category',\n",
    "                                                 target_size=Image_Size,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 batch_size=batch_size, \n",
    "                                                 color_mode='grayscale',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-O5nrz3g-xQd",
    "outputId": "18d1f3b3-f061-4638-81b8-8cb49d08ba9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 958 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Gerando a base de validação\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255) # 1.255\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df, \n",
    "    \"/content/drive/MyDrive/archive/images/gray_images/\", \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=Image_Size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size, \n",
    "    color_mode='grayscale',\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfXYHjIYiihG",
    "outputId": "81b0ef1b-2e38-49d2-c6cc-24466b72dd13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "255/255 [==============================] - 140s 505ms/step - loss: 1.1480 - accuracy: 0.5578 - val_loss: 0.9726 - val_accuracy: 0.4952 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "255/255 [==============================] - 108s 409ms/step - loss: 0.7628 - accuracy: 0.6118 - val_loss: 0.7139 - val_accuracy: 0.5439 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "255/255 [==============================] - 112s 436ms/step - loss: 0.6838 - accuracy: 0.6320 - val_loss: 0.6139 - val_accuracy: 0.6593 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "255/255 [==============================] - 107s 402ms/step - loss: 0.6483 - accuracy: 0.6537 - val_loss: 0.6250 - val_accuracy: 0.6582 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.6290 - accuracy: 0.6684\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "255/255 [==============================] - 95s 373ms/step - loss: 0.6290 - accuracy: 0.6684 - val_loss: 1.1484 - val_accuracy: 0.6063 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "255/255 [==============================] - 109s 426ms/step - loss: 0.5921 - accuracy: 0.6907 - val_loss: 0.5877 - val_accuracy: 0.7037 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "255/255 [==============================] - 115s 449ms/step - loss: 0.5717 - accuracy: 0.7080 - val_loss: 0.5296 - val_accuracy: 0.7249 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "255/255 [==============================] - 110s 430ms/step - loss: 0.5634 - accuracy: 0.7054 - val_loss: 0.5177 - val_accuracy: 0.7397 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "255/255 [==============================] - 94s 366ms/step - loss: 0.5630 - accuracy: 0.7135 - val_loss: 0.6879 - val_accuracy: 0.6201 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.5530 - accuracy: 0.7290\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "255/255 [==============================] - 93s 365ms/step - loss: 0.5530 - accuracy: 0.7290 - val_loss: 0.5640 - val_accuracy: 0.7164 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "255/255 [==============================] - 109s 426ms/step - loss: 0.5285 - accuracy: 0.7465 - val_loss: 0.5196 - val_accuracy: 0.7407 - lr: 2.5000e-04\n",
      "Epoch 12/50\n",
      "255/255 [==============================] - 112s 439ms/step - loss: 0.5188 - accuracy: 0.7429 - val_loss: 0.5088 - val_accuracy: 0.7556 - lr: 2.5000e-04\n",
      "Epoch 13/50\n",
      "255/255 [==============================] - 93s 364ms/step - loss: 0.5109 - accuracy: 0.7410 - val_loss: 0.5169 - val_accuracy: 0.7534 - lr: 2.5000e-04\n",
      "Epoch 14/50\n",
      "255/255 [==============================] - 104s 408ms/step - loss: 0.5059 - accuracy: 0.7520 - val_loss: 0.4996 - val_accuracy: 0.7608 - lr: 2.5000e-04\n",
      "Epoch 15/50\n",
      "255/255 [==============================] - 92s 361ms/step - loss: 0.5019 - accuracy: 0.7549 - val_loss: 0.4842 - val_accuracy: 0.7545 - lr: 2.5000e-04\n",
      "Epoch 16/50\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.4943 - accuracy: 0.7664\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "255/255 [==============================] - 92s 360ms/step - loss: 0.4943 - accuracy: 0.7664 - val_loss: 0.5001 - val_accuracy: 0.7598 - lr: 2.5000e-04\n",
      "Epoch 17/50\n",
      "255/255 [==============================] - 109s 425ms/step - loss: 0.4776 - accuracy: 0.7717 - val_loss: 0.4763 - val_accuracy: 0.7799 - lr: 1.2500e-04\n",
      "Epoch 18/50\n",
      "255/255 [==============================] - 113s 441ms/step - loss: 0.4798 - accuracy: 0.7720 - val_loss: 0.4695 - val_accuracy: 0.7831 - lr: 1.2500e-04\n",
      "Epoch 19/50\n",
      "255/255 [==============================] - 93s 365ms/step - loss: 0.4762 - accuracy: 0.7738 - val_loss: 0.4634 - val_accuracy: 0.7810 - lr: 1.2500e-04\n",
      "Epoch 20/50\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.4746 - accuracy: 0.7751\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "255/255 [==============================] - 93s 365ms/step - loss: 0.4746 - accuracy: 0.7751 - val_loss: 0.4676 - val_accuracy: 0.7767 - lr: 1.2500e-04\n",
      "Epoch 21/50\n",
      "255/255 [==============================] - 110s 430ms/step - loss: 0.4669 - accuracy: 0.7785 - val_loss: 0.4553 - val_accuracy: 0.7852 - lr: 6.2500e-05\n",
      "Epoch 22/50\n",
      "255/255 [==============================] - 112s 439ms/step - loss: 0.4514 - accuracy: 0.7921 - val_loss: 0.4683 - val_accuracy: 0.7862 - lr: 6.2500e-05\n",
      "Epoch 23/50\n",
      "255/255 [==============================] - 107s 420ms/step - loss: 0.4529 - accuracy: 0.7877 - val_loss: 0.4488 - val_accuracy: 0.7884 - lr: 6.2500e-05\n",
      "Epoch 24/50\n",
      "255/255 [==============================] - 108s 423ms/step - loss: 0.4596 - accuracy: 0.7769 - val_loss: 0.4438 - val_accuracy: 0.7937 - lr: 6.2500e-05\n",
      "Epoch 25/50\n",
      "255/255 [==============================] - 98s 384ms/step - loss: 0.4515 - accuracy: 0.7845 - val_loss: 0.4604 - val_accuracy: 0.7905 - lr: 6.2500e-05\n",
      "Epoch 26/50\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.7877\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "255/255 [==============================] - 95s 371ms/step - loss: 0.4535 - accuracy: 0.7877 - val_loss: 0.4512 - val_accuracy: 0.7937 - lr: 6.2500e-05\n",
      "Epoch 27/50\n",
      "255/255 [==============================] - 95s 372ms/step - loss: 0.4391 - accuracy: 0.7934 - val_loss: 0.4553 - val_accuracy: 0.7905 - lr: 3.1250e-05\n",
      "Epoch 28/50\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.4344 - accuracy: 0.7955\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "255/255 [==============================] - 95s 370ms/step - loss: 0.4344 - accuracy: 0.7955 - val_loss: 0.4669 - val_accuracy: 0.7831 - lr: 3.1250e-05\n",
      "Epoch 29/50\n",
      "255/255 [==============================] - 97s 378ms/step - loss: 0.4435 - accuracy: 0.7911 - val_loss: 0.4528 - val_accuracy: 0.7915 - lr: 1.5625e-05\n",
      "Epoch 30/50\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.7924\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "255/255 [==============================] - 96s 377ms/step - loss: 0.4462 - accuracy: 0.7924 - val_loss: 0.4633 - val_accuracy: 0.7820 - lr: 1.5625e-05\n",
      "Epoch 31/50\n",
      "255/255 [==============================] - 96s 374ms/step - loss: 0.4510 - accuracy: 0.7953 - val_loss: 0.4566 - val_accuracy: 0.7905 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "255/255 [==============================] - 113s 443ms/step - loss: 0.4480 - accuracy: 0.7932 - val_loss: 0.4519 - val_accuracy: 0.7989 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "255/255 [==============================] - 99s 388ms/step - loss: 0.4381 - accuracy: 0.7890 - val_loss: 0.4579 - val_accuracy: 0.7884 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "255/255 [==============================] - 95s 372ms/step - loss: 0.4468 - accuracy: 0.7908 - val_loss: 0.4490 - val_accuracy: 0.7968 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "255/255 [==============================] - 95s 373ms/step - loss: 0.4466 - accuracy: 0.7932 - val_loss: 0.4579 - val_accuracy: 0.7905 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "255/255 [==============================] - 97s 381ms/step - loss: 0.4472 - accuracy: 0.7971 - val_loss: 0.4581 - val_accuracy: 0.7937 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "255/255 [==============================] - 96s 376ms/step - loss: 0.4535 - accuracy: 0.7877 - val_loss: 0.4502 - val_accuracy: 0.7989 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "255/255 [==============================] - 110s 432ms/step - loss: 0.4406 - accuracy: 0.7979 - val_loss: 0.4464 - val_accuracy: 0.8011 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "255/255 [==============================] - 97s 381ms/step - loss: 0.4332 - accuracy: 0.7984 - val_loss: 0.4611 - val_accuracy: 0.7894 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "255/255 [==============================] - 93s 366ms/step - loss: 0.4404 - accuracy: 0.7955 - val_loss: 0.4588 - val_accuracy: 0.7947 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "255/255 [==============================] - 93s 364ms/step - loss: 0.4368 - accuracy: 0.8005 - val_loss: 0.4582 - val_accuracy: 0.7937 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "255/255 [==============================] - 93s 363ms/step - loss: 0.4493 - accuracy: 0.7900 - val_loss: 0.4575 - val_accuracy: 0.7947 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "255/255 [==============================] - 93s 363ms/step - loss: 0.4458 - accuracy: 0.7974 - val_loss: 0.4560 - val_accuracy: 0.7979 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "255/255 [==============================] - 93s 363ms/step - loss: 0.4409 - accuracy: 0.7934 - val_loss: 0.4575 - val_accuracy: 0.7905 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Treinamento do modelo\n",
    "epochs=50\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=total_validate//batch_size,\n",
    "    steps_per_epoch=total_train//batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZIx_QKeHrQyx"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/last_model_gray.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6aVZrvpgYeH1"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "#model = keras.models.load_model('/content/drive/MyDrive/last_model_gray.h5')\n",
    "#model = keras.models.load_model('/content/drive/MyDrive/best_model_gray.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qgfo-UkqI0c",
    "outputId": "8b85633b-f689-4615-ea65-59dfaa91aa3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 57s 222ms/step - loss: 0.4054 - accuracy: 0.8151\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(\n",
    "    train_generator,\n",
    "    verbose=1,\n",
    "    max_queue_size=10,\n",
    "    workers=2,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Gm27oSet8cn",
    "outputId": "cbaacc0f-4379-42c1-b6b4-be0668b57a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4053851366043091 | Test accurracy: 0.8151435852050781\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Loss: {evaluation[0]} | Test accurracy: {evaluation[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxiI5MDgOn1v",
    "outputId": "768f1264-b63c-4127-a94a-d90eb9c015b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Matriz de confusão da base de validação\n",
    "predict = model.predict_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Z3R3FoDLliH7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7Zx_F3l18UMX"
   },
   "outputs": [],
   "source": [
    "previsoes = np.argmax(predict, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "g_yjVKXpomOq"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=validate_df['category'].replace({ 'dog': 1, 'cat': 0 }), y_pred=previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "B-L8iOJfakAC"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ExvuQd9F8UMZ"
   },
   "outputs": [],
   "source": [
    "cm_plot_labels = ['Cat','Dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "JGOwoXpi8UMa",
    "outputId": "d9d0148c-cf84-437a-d22d-562402496366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[242 232]\n",
      " [229 255]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "uvQq2uTtRtDm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dogs_and_cats_gray.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
